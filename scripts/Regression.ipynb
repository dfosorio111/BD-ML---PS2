{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using notebook instead.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a7bcbb748dd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import libraries\n",
    "%matplotlib notebook\n",
    "%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import graphviz \n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
    "\n",
    "\n",
    "# metricas de evaluacion para clasificacion (matriz de confusion)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score \n",
    "\n",
    "# metricas de evaluacion para regresion (r2 NO es bueno para el test-set)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/dfdummy.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4268d7ba4bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# cargar la base dummyficada\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_dummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/dfdummy.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# cargar train-set con variables continuas escaladas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/train_s.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/dfdummy.csv'"
     ]
    }
   ],
   "source": [
    "# cargar la base dummyficada\n",
    "df_dummy = pd.read_csv('C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/dfdummy.csv')\n",
    "\n",
    "# cargar train-set con variables continuas escaladas\n",
    "train_s = pd.read_csv('C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/train_s.csv')\n",
    "# cargar test-set con variables continuas escaladas\n",
    "test_s = pd.read_csv('C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/tets_s.csv')\n",
    "\n",
    "# cargar train-set con tecnica de imbalance: undersampling de la clase mayoritaria (NO POBRE)\n",
    "train_s_under = pd.read_csv('C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS2/data/train_s_under.csv')\n",
    "\n",
    "train_s_under.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clase1</th>\n",
       "      <th>Clase2</th>\n",
       "      <th>P5000</th>\n",
       "      <th>P5010</th>\n",
       "      <th>P50902</th>\n",
       "      <th>P50903</th>\n",
       "      <th>P50904</th>\n",
       "      <th>P50905</th>\n",
       "      <th>P50906</th>\n",
       "      <th>...</th>\n",
       "      <th>prop_mujeres_total</th>\n",
       "      <th>Ingtotugarr</th>\n",
       "      <th>recibe_arriendos1</th>\n",
       "      <th>prop_cotiza</th>\n",
       "      <th>ppc</th>\n",
       "      <th>Valor_Arriendo</th>\n",
       "      <th>age2</th>\n",
       "      <th>años_educ_promedio</th>\n",
       "      <th>log_ingtot</th>\n",
       "      <th>Pobre1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.310209</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085933</td>\n",
       "      <td>750000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.855801</td>\n",
       "      <td>-0.877705</td>\n",
       "      <td>-0.059398</td>\n",
       "      <td>0.303971</td>\n",
       "      <td>-0.789602</td>\n",
       "      <td>13.527830</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.289483</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085933</td>\n",
       "      <td>1600000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107525</td>\n",
       "      <td>0.330174</td>\n",
       "      <td>-0.008577</td>\n",
       "      <td>-0.656062</td>\n",
       "      <td>0.085695</td>\n",
       "      <td>14.285515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.289483</td>\n",
       "      <td>2.238883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274556</td>\n",
       "      <td>4498000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.855801</td>\n",
       "      <td>2.443962</td>\n",
       "      <td>-0.046692</td>\n",
       "      <td>-0.246199</td>\n",
       "      <td>-0.664560</td>\n",
       "      <td>15.319144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.909902</td>\n",
       "      <td>-1.100627</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.888376</td>\n",
       "      <td>1234453.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.034178</td>\n",
       "      <td>-0.877705</td>\n",
       "      <td>-0.033987</td>\n",
       "      <td>-0.790548</td>\n",
       "      <td>0.335781</td>\n",
       "      <td>14.026139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.489637</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815288</td>\n",
       "      <td>1173457.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107525</td>\n",
       "      <td>0.330174</td>\n",
       "      <td>-0.033987</td>\n",
       "      <td>-0.560582</td>\n",
       "      <td>0.919312</td>\n",
       "      <td>13.975466</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clase1  Clase2     P5000     P5010  P50902  P50903  P50904  \\\n",
       "0           1       1       0 -0.310209  0.012543       0       0       0   \n",
       "1           2       1       0  1.289483  0.012543       1       0       0   \n",
       "2           3       1       0  1.289483  2.238883       0       0       0   \n",
       "3           4       1       0 -1.909902 -1.100627       0       1       0   \n",
       "4           5       1       0  0.489637  0.012543       0       1       0   \n",
       "\n",
       "   P50905  P50906  ...  prop_mujeres_total  Ingtotugarr  recibe_arriendos1  \\\n",
       "0       0       0  ...           -0.085933     750000.0                  0   \n",
       "1       0       0  ...           -0.085933    1600000.0                  0   \n",
       "2       0       0  ...            0.274556    4498000.0                  0   \n",
       "3       0       0  ...           -1.888376    1234453.0                  0   \n",
       "4       0       0  ...            0.815288    1173457.0                  0   \n",
       "\n",
       "   prop_cotiza       ppc  Valor_Arriendo      age2  años_educ_promedio  \\\n",
       "0    -0.855801 -0.877705       -0.059398  0.303971           -0.789602   \n",
       "1     0.107525  0.330174       -0.008577 -0.656062            0.085695   \n",
       "2    -0.855801  2.443962       -0.046692 -0.246199           -0.664560   \n",
       "3     2.034178 -0.877705       -0.033987 -0.790548            0.335781   \n",
       "4     0.107525  0.330174       -0.033987 -0.560582            0.919312   \n",
       "\n",
       "   log_ingtot  Pobre1  \n",
       "0   13.527830       0  \n",
       "1   14.285515       0  \n",
       "2   15.319144       0  \n",
       "3   14.026139       0  \n",
       "4   13.975466       0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargar bases\n",
    "df_dummy = pd.read_csv('C:/Users/danie/OneDrive/Escritorio/Uniandes/PEG/Big Data and Machine Learning/BD-ML---PS2/data/dfdummy.csv')\n",
    "train_s = pd.read_csv('C:/Users/danie/OneDrive/Escritorio/Uniandes/PEG/Big Data and Machine Learning/BD-ML---PS2/data/train_s.csv')\n",
    "test_s = pd.read_csv('C:/Users/danie/OneDrive/Escritorio/Uniandes/PEG/Big Data and Machine Learning/BD-ML---PS2/data/tets_s.csv')\n",
    "train_s_under = pd.read_csv('C:/Users/danie/OneDrive/Escritorio/Uniandes/PEG/Big Data and Machine Learning/BD-ML---PS2/data/train_s_under.csv')\n",
    "train_s_under.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bases para clasificación\n",
    "# y_train yi obs/target es discreta. variable Pobre1[0,1]\n",
    "# x_train es todas las variables en x_test - y_target(pobre1)- log_ingtot -Ingtotugarr- Clase1- Lp  variables con multi colinealidad\n",
    "\n",
    "y_train = train_s_under['Pobre1']\n",
    "x_train = train_s_under.drop(['Unnamed: 0','Pobre1','log_ingtot','Ingtotugarr','Clase1','Lp'], axis=1) \n",
    "\n",
    "\n",
    "y_test = test_s['Pobre1']\n",
    "x_test = test_s.drop(['Unnamed: 0','Pobre1','log_ingtot','Ingtotugarr','Clase1','Lp'], axis=1) \n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        13.527830\n",
       "1        14.285515\n",
       "2        15.319144\n",
       "3        14.026139\n",
       "4        13.975466\n",
       "           ...    \n",
       "66082    13.307460\n",
       "66083    13.592370\n",
       "66084    12.988830\n",
       "66085    11.945800\n",
       "66086    13.158380\n",
       "Name: log_ingtot, Length: 66087, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bases para regresión\n",
    "# y_train yi obs/target es discreta. variable Pobre1[0,1]\n",
    "# x_train es todas las variables en x_test - y_target(pobre1)- log_ingtot -Ingtotugarr- Clase1- Lp  variables con multi colinealidad\n",
    "\n",
    "y_train = train_s_under['log_ingtot']\n",
    "x_train_lp = train_s_under['Lp']\n",
    "y_train_p = train_s_under['Pobre1']\n",
    "x_train = train_s_under.drop(['Unnamed: 0','Pobre1','log_ingtot','Ingtotugarr','Clase1','Lp'], axis=1) \n",
    "\n",
    "\n",
    "y_test = test_s['log_ingtot']\n",
    "y_test_p = test_s['Pobre1']\n",
    "x_test_lp = test_s['Lp']\n",
    "x_test = test_s.drop(['Unnamed: 0','Pobre1','log_ingtot','Ingtotugarr','Clase1','Lp'], axis=1) \n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8228093270991269 0.8356798617846751 0.7776101956939784 0.5650756247259974 0.7801399659542273 0.7801399659542273 0.7788730266636453 0.6554162495431504\n"
     ]
    }
   ],
   "source": [
    "# crear LinearRegression() Regresion Lineal simple \n",
    "# fit/train ajustar el modelo a train-set\n",
    "\n",
    "linreg1 = LinearRegression().fit(x_train,y_train)\n",
    "\n",
    "\n",
    "# y_hat crear vectores de prediccion y_hat  en train-set y test-set\n",
    "y_hat_train = linreg1.predict(x_train)\n",
    "y_hat_test = linreg1.predict(x_test)\n",
    "\n",
    "# convertir y_hat en discreto\n",
    "\n",
    "y_hat_train_disc = np.exp(y_hat_train)\n",
    "y_hat_train_disc = (y_hat_train_disc/x_train['Npersug'])\n",
    "y_hat_train_resu = y_hat_train_disc <= x_train_lp\n",
    "y_hat_train_resu.astype(int)\n",
    "\n",
    "\n",
    "y_hat_test_disc = np.exp(y_hat_test)\n",
    "y_hat_test_disc = (y_hat_test_disc/x_test['Npersug'])\n",
    "y_hat_test_resu = y_hat_test_disc <= x_test_lp\n",
    "y_hat_test_resu.astype(int)\n",
    "\n",
    "\n",
    "# matriz de confusion\n",
    "confusion_train = confusion_matrix(y_train_p, y_hat_train_resu)\n",
    "confusion_test = confusion_matrix(y_test_p, y_hat_test_resu)\n",
    "\n",
    "\n",
    "# metricas de evaluacion\n",
    "\n",
    "train_acc = accuracy_score(y_train_p, y_hat_train_resu )\n",
    "test_acc = accuracy_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_prec =precision_score(y_train_p, y_hat_train_resu)\n",
    "test_prec =precision_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_recall = recall_score(y_train_p, y_hat_train_resu )\n",
    "test_recall = recall_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_f1 = f1_score(y_train_p, y_hat_train_resu )\n",
    "test_f1 = f1_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "\n",
    "\n",
    "#print(classification_report)\n",
    "print(train_acc, test_acc, train_prec, test_prec, train_recall, test_recall, train_f1, test_f1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización: Ridge y Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El parámetro alpha de regularización óptimo es: 0.0\n",
      "El mínimo Error Cuadrático Medio MSE es: 1.064420295565875\n",
      "El mínimo Error Absoluto Medio MAE es: 0.5374053001732669\n",
      "El ajuste R2 máximo es:  0\n",
      "Ridge\n",
      "ridge regression linear model intercept: 12.675869334865979\n",
      "ridge regression linear model coeff:\n",
      "[-3.97838617e-01  1.56673954e-01 -4.75272846e-02 -1.79958484e-01\n",
      " -3.78100471e-01 -1.48991757e-02 -5.10037463e-01 -3.72537114e-01\n",
      "  3.55322498e-02  1.10827198e-01  1.13881868e-02  1.63487177e-02\n",
      "  5.03579147e-02 -8.58901190e-04 -7.72269043e-02 -2.88887503e-01\n",
      " -1.67086436e-01 -1.79766839e-02 -6.29033261e-02 -4.40380154e-01\n",
      " -3.52613839e-02 -7.24230933e-02  2.39503009e-02 -1.07554100e-01\n",
      " -3.99298582e-02  2.46928341e-02  2.35809255e-02  1.15509737e-01\n",
      "  5.07622247e-02  9.29779155e-02 -1.44440038e-02 -3.40981327e-02\n",
      " -1.43048347e-02  3.68202087e-03 -2.33091249e-02 -1.63608292e-01\n",
      " -3.06707426e-01  2.05493271e-01  2.89575973e-02 -6.12293968e-01\n",
      "  2.68275951e-01  4.00842413e-01  4.64240089e-01  6.78903652e-01\n",
      "  5.51121697e-01  6.09735404e-01  7.32775483e-01  7.60562857e-01\n",
      "  7.47946578e-01  7.50230830e-01  7.40048441e-01  7.49082750e-01\n",
      "  7.69378560e-01 -3.81034286e-02  1.25714195e-01  3.16959393e-01\n",
      "  7.40620305e-02 -6.03291690e-02  1.99028018e-01 -5.63175758e-01\n",
      " -9.32212756e-01  1.05245063e-01 -3.04762044e-01  1.70389144e-01\n",
      " -6.92769460e-02 -4.10872805e-01 -1.18253895e+01  3.31602666e-01\n",
      "  5.91651834e-01  3.32276905e-01  2.93408880e-01  4.70015267e-01\n",
      " -7.54324168e-02 -6.37484920e-02  3.52599433e-01 -6.83777482e-01\n",
      "  2.95119382e-01  2.12059960e-01  3.27942047e-01  2.53572101e-01\n",
      "  9.88892344e-02  2.83252894e-01  2.06995207e-01  1.65957769e-01\n",
      "  2.98071168e-01 -2.09743952e-01  1.35039065e-02  2.64409584e-01\n",
      "  2.26637283e-01  1.37577614e-01  1.10502258e-01  1.60982567e-01\n",
      "  8.75577388e-02 -8.87914903e-02  9.18677554e-02 -5.87037863e-02\n",
      "  4.85751294e-02  7.30378907e-02 -1.55452529e-01  6.72994253e-02\n",
      " -2.36463721e-01  1.42109969e-02  6.67728287e-01  2.76014773e-01\n",
      " -7.65269255e-03  1.58038264e-02  5.87803600e-02]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14784\\3943035318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m      .format(linridge.coef_))\n\u001b[0;32m     93\u001b[0m print('R-squared score (training): {:.3f}'\n\u001b[1;32m---> 94\u001b[1;33m      .format(linridge.score(X_train, y_train)))\n\u001b[0m\u001b[0;32m     95\u001b[0m print('R-squared score (test): {:.3f}'\n\u001b[0;32m     96\u001b[0m      .format(linridge.score(X_test, y_test)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "# NO SE ESCALA PORQUE LAS BASES YA SE EXPORTARON ESCALADAS\n",
    "# RobustScaler  crear escalador Robusto\n",
    "# scaler = RobustScaler()\n",
    "\n",
    "# estandarizar xi DESPUES de partir la base en train-set y test-set\n",
    "# ajustar escalador robusto con x_train\n",
    "#scaler.fit(x_train)\n",
    "\n",
    "# estandarizar x_train y x_test\n",
    "#x_train_scaled = scaler.transform(x_train)\n",
    "#x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# crear RidgeRegression Tunning:  sintonizar parametro alpha de regularizacion\n",
    "\n",
    "#lridge_tunn = Ridge(random_state=0)\n",
    "\n",
    "#alpha_grid = np.linspace(0,1,500)\n",
    "#grid_values_tunn  ={'alpha':alpha_grid}\n",
    "\n",
    "# grid de hiper parámetros para optimizar\n",
    "#grid_lridge_recall =  GridSearchCV(lridge_tunn, param_grid= grid_values_tunn, scoring='recall')\n",
    "#grid_lridge_recall.fit(x_train,y_train)\n",
    "\n",
    "#print('Grid best parameter(max. recall): ', grid_lridge_recall.best_params_)\n",
    "#print('Grid best score (recall): ' , grid_lridge_recall.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linridge_opt = Ridge()\n",
    "\n",
    "\n",
    "alpha_opt=0\n",
    "grid_values_alpha = np.linspace(0,2,num=1000) \n",
    "mse_train_min = 10000\n",
    "mse_test_min = 10000\n",
    "\n",
    "mae_train_min = 10000\n",
    "mae_test_min = 10000\n",
    "\n",
    "r2_train_max = 0\n",
    "r2_test_max = 0\n",
    "\n",
    "\n",
    "grid_values_alpha = alpha_grid\n",
    "\n",
    "for this_alpha in grid_values_alpha:\n",
    "    \n",
    "    # crear Ridge\n",
    "    # fit/train ajustar con train-set\n",
    "    linridge = Ridge(alpha = this_alpha).fit(x_train, y_train)\n",
    "    \n",
    "    # crear vectores de predicción\n",
    "    y_hat_ridge_train = linridge.predict(x_train)\n",
    "    y_hat_ridge_test = linridge.predict(x_test)\n",
    "    \n",
    "    \n",
    "    mae_train = mean_absolute_error(y_hat_ridge_train,y_train)\n",
    "    mae_test = mean_absolute_error(y_hat_ridge_test,y_test)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_hat_ridge_train,y_train)\n",
    "    mse_test = mean_squared_error(y_hat_ridge_test,y_test)\n",
    "    \n",
    "    \n",
    "    r2_train = r2_score(y_hat_ridge_train,y_train)\n",
    "    r2_test = r2_score(y_hat_ridge_test,y_test)\n",
    "    \n",
    "    if mse_test < mse_test_min:\n",
    "        mse_test_min = mse_test\n",
    "        alpha_opt = this_alpha\n",
    "    if mae_test < mae_test_min:\n",
    "        mae_test_min = mae_test\n",
    "    if  r2_test> r2_test_max:\n",
    "        r2_test_max = r2_test \n",
    "\n",
    "print(\"El parámetro alpha de regularización óptimo es: \"+str(alpha_opt) )\n",
    "print(\"El mínimo Error Cuadrático Medio MSE es: \"+str(mse_test_min))\n",
    "print(\"El mínimo Error Absoluto Medio MAE es: \"+str(mae_test_min))\n",
    "print(\"El ajuste R2 máximo es:  \"+str(r2_test_max))\n",
    "\n",
    "\n",
    "linridge = Ridge(alpha=alpha_opt).fit(x_train, y_train)\n",
    "\n",
    "\n",
    "print('Ridge')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(x_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(x_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat crear vectores de prediccion y_hat  en train-set y test-set\n",
    "y_hat_train = linreg1.predict(x_train)\n",
    "y_hat_test = linreg1.predict(x_test)\n",
    "\n",
    "# convertir y_hat en continuo\n",
    "\n",
    "y_hat_train_disc = np.exp(y_hat_train)\n",
    "y_hat_train_disc = (y_hat_train_disc/x_train['Npersug'])\n",
    "y_hat_train_resu = y_hat_train_disc <= x_train_lp\n",
    "y_hat_train_resu.astype(int)\n",
    "\n",
    "\n",
    "y_hat_test_disc = np.exp(y_hat_test)\n",
    "y_hat_test_disc = (y_hat_test_disc/x_test['Npersug'])\n",
    "y_hat_test_resu = y_hat_test_disc <= x_test_lp\n",
    "y_hat_test_resu.astype(int)\n",
    "\n",
    "\n",
    "# matriz de confusion\n",
    "confusion_train = confusion_matrix(y_train_p, y_hat_train_resu)\n",
    "confusion_test = confusion_matrix(y_test_p, y_hat_test_resu)\n",
    "\n",
    "# metricas de evaluacion\n",
    "train_acc = accuracy_score(y_train_p, y_hat_train_resu )\n",
    "test_acc = accuracy_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_prec =precision_score(y_train_p, y_hat_train_resu)\n",
    "test_prec =precision_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_recall = recall_score(y_train_p, y_hat_train_resu )\n",
    "test_recall = recall_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_f1 = f1_score(y_train_p, y_hat_train_resu )\n",
    "test_f1 = f1_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "\n",
    "\n",
    "#print(classification_report)\n",
    "print(train_acc, test_acc, train_prec, test_prec, train_recall, test_recall, train_f1, test_f1)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print('Ridge regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('Ridge regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression \n",
    "\n",
    "# RobustScaler  crear escalador Robusto\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "# estandarizar xi DESPUES de partir la base en train-set y test-set\n",
    "# ajustar escalador robusto con x_train\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# estandarizar x_train y x_test\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# crear RidgeRegression ,fit/train ajustar modelo con train-set\n",
    "\n",
    "lasso = Lasso(random_state=0).fit(x_train_scaled, y_train)\n",
    "lasso\n",
    "\n",
    "alpha_reg = np.linspace(0,1,100)\n",
    "\n",
    "\n",
    "# crear grid de valores para hacer GridSerachCV de parametros optimos mediante validacion cruzada\n",
    "grid_values = {'alpha': alpha_reg }\n",
    "\n",
    "# crear GridSearchCV \n",
    "grid_lasso = GridSearchCV(lasso, param_grid= grid_values, scoring='recall')\n",
    "grid_lasso.fit(x_train_scaled,y_train)\n",
    "\n",
    "print('Grid best parameter(max. recall): ', grid_lasso.best_params_)\n",
    "print('Grid best score (recall): ' , grid_lasso.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat crear vectores de prediccion y_hat  en train-set y test-set\n",
    "y_hat_train = linreg1.predict(x_train)\n",
    "y_hat_test = linreg1.predict(x_test)\n",
    "\n",
    "# convertir y_hat en continuo\n",
    "\n",
    "y_hat_train_disc = np.exp(y_hat_train)\n",
    "y_hat_train_disc = (y_hat_train_disc/x_train['Npersug'])\n",
    "y_hat_train_resu = y_hat_train_disc <= x_train_lp\n",
    "y_hat_train_resu.astype(int)\n",
    "\n",
    "\n",
    "y_hat_test_disc = np.exp(y_hat_test)\n",
    "y_hat_test_disc = (y_hat_test_disc/x_test['Npersug'])\n",
    "y_hat_test_resu = y_hat_test_disc <= x_test_lp\n",
    "y_hat_test_resu.astype(int)\n",
    "\n",
    "\n",
    "# matriz de confusion\n",
    "confusion_train = confusion_matrix(y_train_p, y_hat_train_resu)\n",
    "confusion_test = confusion_matrix(y_test_p, y_hat_test_resu)\n",
    "\n",
    "# metricas de evaluacion\n",
    "train_acc = accuracy_score(y_train_p, y_hat_train_resu )\n",
    "test_acc = accuracy_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_prec =precision_score(y_train_p, y_hat_train_resu)\n",
    "test_prec =precision_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_recall = recall_score(y_train_p, y_hat_train_resu )\n",
    "test_recall = recall_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "train_f1 = f1_score(y_train_p, y_hat_train_resu )\n",
    "test_f1 = f1_score(y_test_p, y_hat_test_resu )\n",
    "\n",
    "\n",
    "#print(classification_report)\n",
    "print(train_acc, test_acc, train_prec, test_prec, train_recall, test_recall, train_f1, test_f1)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresion Logistica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RobustScaler  crear escalador Robusto\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "# estandarizar xi DESPUES de partir la base en train-set y test-set\n",
    "# ajustar escalador robusto con x_train\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# estandarizar x_train y x_test\n",
    "#x_train_scaled = scaler.transform(x_train)\n",
    "#x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# crear modelo Logistica (MLE para ajustar parametros/betas)\n",
    "log = LogisticRegression().fit(x_train,y_train_p)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
